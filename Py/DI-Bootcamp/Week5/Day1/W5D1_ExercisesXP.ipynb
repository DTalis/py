{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ebaf841d",
      "metadata": {
        "id": "ebaf841d"
      },
      "source": [
        "# Exercises XP: Student Notebook\n",
        "\n",
        "For each exercise, the **Instructions** from the plateform are guided, and the **Guidance** explains exactly what you must do to complete the task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2c0f244",
      "metadata": {
        "id": "f2c0f244"
      },
      "source": [
        "## What you will learn\n",
        "- How to clearly define and articulate a machine learning problem statement.\n",
        "\n",
        "- The process of data collection, including identifying relevant data types and potential data sources.\n",
        "Skills in feature selection and justification for machine learning models, particularly in the context of loan default prediction.\n",
        "\n",
        "- Understanding of different types of machine learning models and their suitability for various real-world scenarios.\n",
        "\n",
        "- Techniques and strategies for evaluating the performance of different machine learning models, including choosing appropriate metrics and understanding their implications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17aa83e3",
      "metadata": {
        "id": "17aa83e3"
      },
      "source": [
        "## What you will create\n",
        "- A detailed problem statement and data collection plan for a loan default prediction project, including identification of key data types and sources.\n",
        "- A comprehensive feature selection analysis for a hypothetical loan default prediction dataset.\n",
        "- A theoretical evaluation strategy for three different types of machine learning models, addressing the unique challenges and metrics relevant to each model type.\n",
        "- Thoughtful analyses and justifications for choosing specific machine learning approaches for varied scenarios such as stock price prediction, library organization, and robot navigation.\n",
        "- A document or presentation that showcases your understanding and approach to evaluating and optimizing machine learning models in diverse contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93e965f9",
      "metadata": {
        "id": "93e965f9"
      },
      "source": [
        "## ðŸŒŸ Exercise 1 : Defining the Problem and Data Collection for Loan Default Prediction\n",
        "\n",
        "### Instructions\n",
        "- Write a clear problem statement for predicting loan defaults.\n",
        "- Identify and list the types of data you would need for this project (e.g., personal details of applicants, credit scores, loan amounts, repayment history).\n",
        "- Discuss the sources where you can collect this data (e.g., financial institutionâ€™s internal records, credit bureaus).\n",
        "\n",
        "**Expected Output:** A document detailing the problem statement and a comprehensive plan for data collection, including data types and sources."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62ea95bd",
      "metadata": {
        "id": "62ea95bd"
      },
      "source": [
        "### Guidance\n",
        "- Please write your answer as a short document. Begin by stating the prediction objective in a complete sentence that names the target variable and the decision it will support. Then, describe the data types you would collect in complete sentences. For each data type, explain in one sentence why it could help predict loan defaults.\n",
        "\n",
        "- After that, name realistic data sources in complete sentences, and briefly describe how you would obtain or integrate each source.\n",
        "\n",
        "- Finally, include one paragraph that explains risks and constraints such as privacy, regulation, data quality, sampling bias, and governance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b165d7a",
      "metadata": {
        "id": "9b165d7a"
      },
      "source": [
        "### Your answer\n",
        "Problem statement (objective and decision)\n",
        "\n",
        "We will predict the probability of loan default within 90 days past due (target variable: default_90dpd = 1/0) at or before loan approval, so that credit officers can approve, price, or decline applications and set appropriate credit limits and monitoring rules.\n",
        "\n",
        "Data to collect (what and why)\n",
        "\t1.\tApplicant demographics (age, residency status, marital status, dependents). These features provide context for stability and potential financial obligations that can correlate with repayment risk.\n",
        "\t2.\tEmployment details (employment status, tenure, occupation, employer sector). Job stability and industry cyclicality are predictive of income continuity and default likelihood.\n",
        "\t3.\tIncome and affordability (gross/net monthly income, other income sources, DTI ratio). Ability-to-pay signalsâ€”especially debt-to-incomeâ€”directly relate to the capacity to service new debt.\n",
        "\t4.\tCredit bureau history (credit score, number of active trades, utilization, delinquencies, inquiries, bankruptcies). Past credit behavior is a strong leading indicator of future repayment performance.\n",
        "\t5.\tLoan application characteristics (amount, term, product type, interest rate, collateral, purpose). Loan structure and purpose affect payment burden and incentives to repay.\n",
        "\t6.\tBanking behavior (account balances, inflow/outflow volatility, returned payments). Cash-flow patterns capture real affordability and financial stress beyond stated income.\n",
        "\t7.\tRepayment history on internal products (previous loans/cards with the institution, days past due, restructurings). In-house historical performance provides high-fidelity signals about the applicantâ€™s behavior with this lender.\n",
        "\t8.\tCollateral/guarantor details (collateral value/LTV, guarantor credit strength). Security and support reduce loss severity and can lower default propensity.\n",
        "\t9.\tApplication channel and fraud indicators (channel type, device fingerprint, mismatch flags, velocity). Certain channels and inconsistencies correlate with higher first-payment default and fraud risk.\n",
        "\t10.\tMacroeconomic context (unemployment rate, inflation, interest rates, region). External conditions influence default rates and help the model generalize across cycles.\n",
        "\n",
        "Data sources and integration approach\n",
        "\t1.\tFinancial institution internal systems (LOS/LMS, core banking, collections). Extract application, loan, and repayment data via batch ETL or streaming into the data warehouse; join on customer ID and loan ID.\n",
        "\t2.\tCredit bureaus. Pull scores and tradeline attributes through bureau APIs/SFTP with applicant consent; store snapshots with bureau date to manage staleness.\n",
        "\t3.\tOpen Banking / bank-statement providers (with consent). Ingest categorized transactions and balance histories via secure APIs to compute cash-flow and affordability features.\n",
        "\t4.\tFraud and identity verification vendors. Integrate device, document, and consortium fraud signals through real-time APIs to enrich first-party data.\n",
        "\t5.\tCollateral valuation sources (internal appraisals or third-party AVMs). Load valuations and compute LTV metrics alongside loan terms.\n",
        "\t6.\tPublic or benchmark datasets for prototyping (e.g., historical lending datasets) and macroeconomic data (national statistics or central bank feeds). Use these for feature engineering ideas and macro join-keys by date/region; keep distinct from production data and document provenance.\n",
        "\n",
        "Risks and constraints (privacy, quality, bias, governance)\n",
        "\n",
        "This project must comply with privacy and data-protection laws (e.g., GDPR/CCPA and local regulations), obtain explicit customer consent for bureau and bank-statement pulls, and minimize/secure personally identifiable information via encryption, access controls, and retention limits. Data quality risks include missing or stale bureau pulls, duplicate identities, and inconsistent IDs across systems; we will implement validation rules, survivorship logic, and time-stamped snapshots. Sampling bias (e.g., only past approved applicants) can skew results; we will correct with reject inference or careful evaluation to avoid optimism. Fair-lending and discrimination risks require excluding protected attributes and testing for disparate impact, while maintaining model explainability for adverse-action notices. Finally, we will operate under model-risk management and governanceâ€”documenting assumptions, versioning data and code, monitoring drift/calibration, and establishing periodic re-training and review."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c297407",
      "metadata": {
        "id": "7c297407"
      },
      "source": [
        "## ðŸŒŸ Exercise 2 : Feature Selection and Model Choice for Loan Default Prediction\n",
        "\n",
        "### Instructions\n",
        "From this dataset, identify which features might be most relevant for predicting loan defaults.\n",
        "Justify your choice of features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4deb8c95",
      "metadata": {
        "id": "4deb8c95"
      },
      "source": [
        "### Guidance\n",
        "- First, identify the features that you believe are most relevant, and write their names in a sentence.\n",
        "Then, provide a justification in complete sentences that explains how each selected feature relates to the likelihood of default.\n",
        "\n",
        "- If you decide to exclude common features, write one sentence for each excluded feature to explain why it is not appropriate in this context.\n",
        "\n",
        "- Conclude with two complete sentences that explain how you would encode categorical features and how you would impute missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3af9600b",
      "metadata": {
        "id": "3af9600b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8b6d453-a8e0-4fe8-e7ec-2c788e39c119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You will now justify the selected features in complete sentences below.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# This piece of code is already prefilled, run it to execute it and see the results.\n",
        "# It provides a simple template you can modify while writing your justification.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# This placeholder DataFrame allows the cell to run even if you did not load a dataset yet.\n",
        "example_columns = [\n",
        "    \"age\",\"employment_length\",\"annual_income\",\"credit_score\",\"loan_amount\",\"interest_rate\",\n",
        "    \"debt_to_income\",\"num_delinquencies\",\"num_open_accounts\",\"total_utilization\",\"home_ownership\",\n",
        "    \"purpose\",\"term\",\"application_type\",\"state\",\"zip_code\"\n",
        "]\n",
        "df = pd.DataFrame(columns=example_columns)\n",
        "\n",
        "# Please replace this list with the actual columns that you select.\n",
        "selected_features = [\n",
        "    # e.g., \"credit_score\",\"debt_to_income\",\"annual_income\",\"loan_amount\",\"interest_rate\",\n",
        "    # \"employment_length\",\"num_delinquencies\",\"total_utilization\"\n",
        "]\n",
        "\n",
        "print(\"You will now justify the selected features in complete sentences below.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed0e3a74",
      "metadata": {
        "id": "ed0e3a74"
      },
      "source": [
        "### Your justification\n",
        "the features most relevant for predicting loan defaults (Loan_Status) are: ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term, Credit_History, Dependents, Education, Self_Employed, Married, Gender, and Property_Area.\n",
        "\n",
        "ApplicantIncome is relevant because higher or more stable income generally improves a borrowerâ€™s ability to meet monthly repayment obligations, reducing default risk.\n",
        "CoapplicantIncome matters because combined household income can significantly strengthen loan affordability and reduce financial stress.\n",
        "LoanAmount is important because larger loan sizes increase the repayment burden, which may elevate the likelihood of default, especially for applicants with lower income.\n",
        "Loan_Amount_Term affects default risk because the repayment period determines monthly installment size; shorter terms increase monthly payments and may raise default probability.\n",
        "Credit_History is one of the strongest predictors because borrowers with a clean repayment record are statistically more likely to meet their obligations on a new loan.\n",
        "Dependents contribute useful information because applicants with more dependents typically face higher household expenses, potentially worsening repayment capacity.\n",
        "Education helps capture differences in long-term earning potential and financial stability, making it indirectly related to loan repayment performance.\n",
        "Self_Employed status is relevant because self-employment incomes tend to be more volatile, which can increase repayment uncertainty.\n",
        "Married status may impact financial stability, because dual-income households may be more resilient and better able to manage unexpected expenses.\n",
        "Gender can be included because it is part of the dataset, but it should be used with caution, as it may not be ethically or legally appropriate in real credit models; however, in this academic dataset it may still contribute to prediction power.\n",
        "Property_Area is relevant because urban, semi-urban, and rural areas differ in income opportunities, stability, and loan pricing, all of which influence repayment behavior.\n",
        "\n",
        "Excluded Feature\n",
        "\n",
        "Loan_ID is excluded because it is purely an identifier and contains no predictive value related to repayment behavior.\n",
        "\n",
        "Encoding and Missing Values\n",
        "\n",
        "Categorical features such as Gender, Married, Education, and Property_Area would be encoded using one-hot encoding to convert them into numerical indicators suitable for machine-learning algorithms.\n",
        "Missing values in numerical variables such as LoanAmount or Loan_Amount_Term would be imputed using median imputation, while missing categorical values would be filled using the mode to preserve the most common category."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e8e6708",
      "metadata": {
        "id": "9e8e6708"
      },
      "source": [
        "## ðŸŒŸ Exercise 3 : Training, Evaluating, and Optimizing the Model\n",
        "\n",
        "### Instructions\n",
        "Which model(s) would you pick for a Loan Prediction ?\n",
        "Outline the steps to evaluate the modelâ€™s performance, mentioning specific metrics that would be relevant to evaluate the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe22b7ce",
      "metadata": {
        "id": "fe22b7ce"
      },
      "source": [
        "### Guidance\n",
        "- Begin by naming one or two candidate models in a complete sentence and explain why each model is suitable for this problem.\n",
        "\n",
        "- Next, describe an evaluation plan in complete sentences that covers the data split, the cross-validation strategy, the metrics you will report, and how you will choose a decision threshold.\n",
        "\n",
        "- Then, explain in complete sentences how you will address class imbalance using stratification, class weights, or resampling.\n",
        "\n",
        "- Finally, state in one or two complete sentences how you would iterate on hyperparameters to improve performance while avoiding data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d630ee52",
      "metadata": {
        "id": "d630ee52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d080eca-2730-4211-ef7b-9bae68443d4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n",
            "ROC-AUC: 1.0\n",
            "PR-AUC (Average Precision): 1.0\n",
            "\n",
            "Confusion matrix:\n",
            " [[6 0]\n",
            " [0 4]]\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         6\n",
            "           1       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           1.00        10\n",
            "   macro avg       1.00      1.00      1.00        10\n",
            "weighted avg       1.00      1.00      1.00        10\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# This piece of code is already prefilled, run it to execute it and see the results.\n",
        "# It demonstrates standard classification metrics for binary loan default prediction.\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix, classification_report\n",
        "\n",
        "# Please replace these placeholders with your true labels and predicted probabilities.\n",
        "y_true = [0,1,0,1,0,0,1,0,1,0]            # placeholder labels\n",
        "y_pred_proba = [0.05,0.80,0.10,0.65,0.20,0.15,0.70,0.30,0.85,0.25]  # placeholder probabilities\n",
        "\n",
        "# You should set a decision threshold that reflects the precisionâ€“recall trade-off for your business case.\n",
        "threshold = 0.5\n",
        "y_pred = [1 if p >= threshold else 0 for p in y_pred_proba]\n",
        "\n",
        "print(\"Accuracy:\", round(accuracy_score(y_true, y_pred), 4))\n",
        "print(\"Precision:\", round(precision_score(y_true, y_pred, zero_division=0), 4))\n",
        "print(\"Recall:\", round(recall_score(y_true, y_pred, zero_division=0), 4))\n",
        "print(\"F1-score:\", round(f1_score(y_true, y_pred, zero_division=0), 4))\n",
        "print(\"ROC-AUC:\", round(roc_auc_score(y_true, y_pred_proba), 4))\n",
        "print(\"PR-AUC (Average Precision):\", round(average_precision_score(y_true, y_pred_proba), 4))\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_true, y_pred, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdd4054c",
      "metadata": {
        "id": "cdd4054c"
      },
      "source": [
        "### Your answer\n",
        "I would select Logistic Regression and Random Forest Classifier as the two primary candidate models. Logistic Regression is suitable because it is interpretable, works well on structured tabular data, and provides calibrated probability estimates that are essential for credit-risk decisions. Random Forest is suitable because it can capture nonlinear relationships and interactions between variables, is robust to outliers and missingness, and typically performs strongly on medium-sized structured datasets like loan applications.\n",
        "\n",
        "To evaluate these models, I would begin by splitting the dataset into training (70â€“80%) and testing (20â€“30%) sets using a stratified split to preserve the proportion of approved versus rejected loans. I would use k-fold cross-validation (typically k=5 or k=10) on the training set to obtain stable performance estimates and reduce variance. The main evaluation metrics would include accuracy, precision, recall, F1-score, ROC-AUC, and PR-AUC, because these metrics collectively capture both overall correctness and the ability to detect high-risk borrowers. I would choose the final decision threshold by analyzing the precision-recall trade-off and selecting a cutoff that aligns with business prioritiesâ€”for example, prioritizing recall if the institution wants to minimize approving risky customers.\n",
        "\n",
        "To address class imbalance, I would apply stratified sampling during all train/test splits and cross-validation folds to maintain consistent class proportions. Additionally, I would experiment with class weights (for example, class_weight=\"balanced\") during model training or apply resampling techniques such as SMOTE or random under-sampling to ensure that minority-class signals are learned effectively.\n",
        "\n",
        "Finally, I would tune hyperparameters using grid search or randomized search applied only to the training folds, ensuring that the test set remains untouched to avoid data leakage. Hyperparameter tuning would include parameters such as regularization strength for Logistic Regression or tree depth and number of estimators for Random Forest, iteratively improving performance without exposing the model to unseen test labels."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0488eb67",
      "metadata": {
        "id": "0488eb67"
      },
      "source": [
        "## ðŸŒŸ Exercise 4 : Designing Machine Learning Solutions for Specific Problems\n",
        "\n",
        "### Instructions\n",
        "For each of these scenario, decide which type of machine learning would be most suitable. Explain.\n",
        "\n",
        "Predicting Stock Prices : predict future prices\n",
        "Organizing a Library of Books : group books into genres or categories based on similarities.\n",
        "Program a robot to navigate and find the shortest path in a maze."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00c00056",
      "metadata": {
        "id": "00c00056"
      },
      "source": [
        "### Guidance\n",
        "Please identify the appropriate machine learning paradigm for each scenario in complete sentences and justify your choice.\n",
        "\n",
        "For each scenario, write one complete sentence that describes the input data, one complete sentence that describes the output, and one complete sentence that describes the learning signal or objective."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d27bd365",
      "metadata": {
        "id": "d27bd365"
      },
      "source": [
        "### Your answer\n",
        "1. Predicting Stock Prices\n",
        "\n",
        "Machine-learning paradigm:\n",
        "This problem is best solved using supervised learning, because we have historical labeled data where past prices are known and the model is trained to predict future numeric values.\n",
        "\n",
        "Input data:\n",
        "The input data would consist of historical features such as past stock prices, trading volume, technical indicators, and possibly external financial variables.\n",
        "\n",
        "Output:\n",
        "The output would be a continuous numerical prediction of the stockâ€™s future price for a given time horizon.\n",
        "\n",
        "Learning signal / objective:\n",
        "The learning objective would be to minimize a regression loss function such as mean squared error so that predicted future prices closely match the true future prices.\n",
        "\n",
        "â¸»\n",
        "\n",
        "2. Organizing a Library of Books\n",
        "\n",
        "Machine-learning paradigm:\n",
        "This scenario is well suited for unsupervised learning, because the goal is to automatically group books into categories based on inherent similarities without requiring predefined labels.\n",
        "\n",
        "Input data:\n",
        "The input data would include text descriptions, book summaries, metadata, or embeddings derived from book content.\n",
        "\n",
        "Output:\n",
        "The output would be clusters of books where each group contains items that are similar in theme, style, or topic.\n",
        "\n",
        "Learning signal / objective:\n",
        "The learning objective is to identify natural structure in the data and maximize within-cluster similarity while maximizing separation between clusters.\n",
        "\n",
        "â¸»\n",
        "\n",
        "3. Programming a Robot to Navigate a Maze\n",
        "\n",
        "Machine-learning paradigm:\n",
        "This scenario is best addressed using reinforcement learning, because the robot must learn through trial and error to choose actions that lead to the shortest path and maximize cumulative reward.\n",
        "\n",
        "Input data:\n",
        "The input data would consist of the robotâ€™s sensory observations or its current position within the maze environment.\n",
        "\n",
        "Output:\n",
        "The output would be an action at each stepâ€”such as moving forward, turning left, or turning rightâ€”that determines how the robot navigates.\n",
        "\n",
        "Learning signal / objective:\n",
        "The learning objective is to maximize rewards by reaching the goal efficiently, which encourages the robot to learn optimal policies for finding the shortest path."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e8e44a7",
      "metadata": {
        "id": "2e8e44a7"
      },
      "source": [
        "## ðŸŒŸ Exercise 5 : Designing an Evaluation Strategy for Different ML Models\n",
        "\n",
        "### Instructions\n",
        "- Select three types of machine learning models: one from supervised learning (e.g., a classification model), one from unsupervised learning (e.g., a clustering model), and one from reinforcement learning. - For the supervised model, outline a strategy to evaluate its performance, including the choice of metrics (like accuracy, precision, recall, F1-score) and methods (like cross-validation, ROC curves).\n",
        "- For the unsupervised model, describe how you would assess the effectiveness of the model, considering techniques like silhouette score, elbow method, or cluster validation metrics.\n",
        "- For the reinforcement learning model, discuss how you would measure its success, considering aspects like cumulative reward, convergence, and exploration vs. exploitation balance.\n",
        "- Address the challenges and limitations of evaluating models in each category."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f13dd9",
      "metadata": {
        "id": "f6f13dd9"
      },
      "source": [
        "### Guidance\n",
        "- Please write a separate paragraph for each of the three model categories.\n",
        "- In the supervised paragraph, describe your validation plan and list the metrics you will report in complete sentences.\n",
        "- In the unsupervised paragraph, explain how you would measure cluster quality or structure in complete sentences and mention any diagnostic plots.\n",
        "- In the reinforcement learning paragraph, describe how you would track cumulative reward, assess convergence, and balance exploration and exploitation using complete sentences.\n",
        "Conclude with one complete sentence per category that states a key evaluation challenge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ff6d0257",
      "metadata": {
        "id": "ff6d0257",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "032656a1-8473-4a25-bc3e-c345f5ef46ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n",
            "ROC-AUC: 1.0\n",
            "PR-AUC (Average Precision): 1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# This piece of code is already prefilled, run it to execute it and see the results.\n",
        "# Supervised classification metrics template with placeholders.\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
        "\n",
        "# Replace these placeholders with your real outputs.\n",
        "y_true = [0,1,1,0,1,0,0,1,0,1]\n",
        "y_pred_proba = [0.1,0.7,0.8,0.2,0.6,0.3,0.4,0.9,0.2,0.85]\n",
        "threshold = 0.5\n",
        "y_pred = [1 if p >= threshold else 0 for p in y_pred_proba]\n",
        "\n",
        "print(\"Accuracy:\", round(accuracy_score(y_true, y_pred), 4))\n",
        "print(\"Precision:\", round(precision_score(y_true, y_pred, zero_division=0), 4))\n",
        "print(\"Recall:\", round(recall_score(y_true, y_pred, zero_division=0), 4))\n",
        "print(\"F1-score:\", round(f1_score(y_true, y_pred, zero_division=0), 4))\n",
        "print(\"ROC-AUC:\", round(roc_auc_score(y_true, y_pred_proba), 4))\n",
        "print(\"PR-AUC (Average Precision):\", round(average_precision_score(y_true, y_pred_proba), 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cf3c51e0",
      "metadata": {
        "id": "cf3c51e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dadfc3e-0e84-4000-8d37-4f3c56a14d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette score (higher is better): 0.848\n",
            "Please explain in complete sentences when you would use the elbow method and how you would interpret it.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# This piece of code is already prefilled, run it to execute it and see the results.\n",
        "# Unsupervised clustering metrics template with synthetic data.\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "kmeans = KMeans(n_clusters=3, n_init=\"auto\", random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "sil = silhouette_score(X, labels)\n",
        "print(\"Silhouette score (higher is better):\", round(sil, 4))\n",
        "\n",
        "print(\"Please explain in complete sentences when you would use the elbow method and how you would interpret it.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7d9f0ef",
      "metadata": {
        "id": "e7d9f0ef"
      },
      "source": [
        "### Your answer\n",
        "Supervised Learning (e.g., Classification Model)\n",
        "\n",
        "For a supervised learning model such as a loan default classifier, I would evaluate performance by splitting the dataset into training and testing sets using stratified sampling, and then applying k-fold cross-validation on the training set to obtain stable and unbiased performance estimates. I would report several evaluation metrics including accuracy for overall correctness, precision to measure the reliability of identifying defaulters, recall to measure how many true defaulters are captured, and F1-score to summarize the balance between precision and recall. I would also generate a ROC curve to visualize the true-positive/false-positive trade-off and compute the ROC-AUC score as a threshold-independent measure of model discrimination. A key challenge in evaluating supervised models is dealing with class imbalance, which can cause misleading accuracy scores and require careful threshold tuning.\n",
        "\n",
        "Unsupervised Learning (e.g., Clustering Model)\n",
        "\n",
        "For an unsupervised learning model such as K-Means clustering, I would assess effectiveness by computing internal validation metrics that quantify how well the discovered clusters represent meaningful structure. I would use the silhouette score to measure how similar each sample is to its assigned cluster relative to other clusters, and I would apply the elbow method by plotting the within-cluster sum of squared distances to identify the optimal number of clusters. I would also inspect cluster centroids or dendrograms (if using hierarchical clustering) to evaluate whether the cluster assignments align with intuitive or interpretable groupings. A central challenge in evaluating unsupervised models is the absence of ground-truth labels, which makes it difficult to determine whether the discovered structure is meaningful or merely an artifact of the algorithm.\n",
        "\n",
        "Reinforcement Learning (e.g., Navigation or Control Problem)\n",
        "\n",
        "For a reinforcement learning model such as a maze-navigation agent, I would measure performance by tracking the cumulative reward obtained across episodes, since higher cumulative reward indicates more successful policies. I would assess convergence by monitoring whether the reward curve stabilizes over time, suggesting that the agent has learned a near-optimal policy. I would also evaluate the balance between exploration and exploitation by analyzing the agentâ€™s behavior, such as the frequency of random exploratory actions versus policy-driven decisions, and adjusting parameters like epsilon in an epsilon-greedy strategy when necessary. A key challenge in evaluating reinforcement learning models is that performance can vary greatly between episodes, making it difficult to determine true convergence or stability."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}