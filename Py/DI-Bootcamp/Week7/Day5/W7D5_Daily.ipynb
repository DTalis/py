{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvqbh2qREut3",
        "outputId": "2d5b11ba-c953-41a3-dc52-74c6b62f483a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting GitHub Topics Scraper...\n",
            "--- 1. Fetching HTML from https://github.com/topics ---\n",
            "Status Code: 200\n",
            "Request successful.\n",
            "First 100 characters:\n",
            "\n",
            "\n",
            "<!DOCTYPE html>\n",
            "<html\n",
            "  lang=\"en\"\n",
            "  \n",
            "  data-color-mode=\"auto\" data-light-theme=\"light\" data-dark-t\n",
            "--------------------------------------------------\n",
            "Successfully saved HTML content to webpage.html\n",
            "\n",
            "--- 2. Parsing webpage.html with BeautifulSoup ---\n",
            "\n",
            "--- Extracted Titles ---\n",
            "Found 16 titles.\n",
            "  1. Awesome Lists\n",
            "  2. Chrome\n",
            "  3. Code quality\n",
            "  4. Compiler\n",
            "  5. CSS\n",
            "\n",
            "--- Extracted Descriptions ---\n",
            "Found 16 descriptions.\n",
            "  1. An awesome list is a list of awesome things curated by the community.\n",
            "  2. Chrome is a web browser from the tech company Google.\n",
            "  3. Automate your code review with style, quality, security, and testâ€‘coverage checks when you need them.\n",
            "  4. Compilers are software that translate higher-level programming languages to lower-level languages (e.g. machine code).\n",
            "  5. Cascading Style Sheets (CSS) is a language used most often to style and improve upon the appearance of views.\n",
            "\n",
            "--- 3. Creating pandas DataFrame ---\n",
            "Successfully created DataFrame:\n",
            "                 Title                                        Description\n",
            "0        Awesome Lists  An awesome list is a list of awesome things cu...\n",
            "1               Chrome  Chrome is a web browser from the tech company ...\n",
            "2         Code quality  Automate your code review with style, quality,...\n",
            "3             Compiler  Compilers are software that translate higher-l...\n",
            "4                  CSS  Cascading Style Sheets (CSS) is a language use...\n",
            "5             Database  A database is a structured set of data held in...\n",
            "6            Front end  Front end is the programming and layout that p...\n",
            "7           JavaScript  JavaScript (JS) is a lightweight interpreted p...\n",
            "8              Node.js  Node.js is a tool for executing JavaScript in ...\n",
            "9                  npm  npm is a package manager for JavaScript includ...\n",
            "10  Project management  Project management is about building scope and...\n",
            "11              Python  Python is a dynamically typed programming lang...\n",
            "12               React  React is an open source JavaScript library use...\n",
            "13        React Native  React Native is a JavaScript mobile framework ...\n",
            "14               Scala  Scala is an object-oriented programming language.\n",
            "15          TypeScript  TypeScript is a typed superset of JavaScript t...\n",
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 16 entries, 0 to 15\n",
            "Data columns (total 2 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Title        16 non-null     object\n",
            " 1   Description  16 non-null     object\n",
            "dtypes: object(2)\n",
            "memory usage: 388.0+ bytes\n",
            "\n",
            "Scraping process complete.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os # Import os to manage file paths\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "URL = \"https://github.com/topics\"\n",
        "HTML_FILE = \"webpage.html\"\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "def fetch_and_save_html(url, file_path):\n",
        "    \"\"\"Fetches HTML content from a URL and saves it to a file.\"\"\"\n",
        "    print(f\"--- 1. Fetching HTML from {url} ---\")\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "\n",
        "        # 1.1. Print status code\n",
        "        print(f\"Status Code: {response.status_code}\")\n",
        "\n",
        "        # 1.2. Check for successful request\n",
        "        response.raise_for_status() # Raises an error for bad status codes\n",
        "        print(\"Request successful.\")\n",
        "\n",
        "        # 1.3. Print first 100 characters\n",
        "        html_content = response.text\n",
        "        print(f\"First 100 characters:\\n{html_content[:100]}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # 1.4. Save content to file with correct encoding\n",
        "        with open(file_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(html_content)\n",
        "        print(f\"Successfully saved HTML content to {file_path}\")\n",
        "        return True\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return False\n",
        "\n",
        "def parse_html_file(file_path):\n",
        "    \"\"\"Parses a local HTML file and extracts topic titles and descriptions.\"\"\"\n",
        "    print(f\"\\n--- 2. Parsing {file_path} with BeautifulSoup ---\")\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            html_content = f.read()\n",
        "\n",
        "        # 2.1. Create BeautifulSoup object\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # 2.2. Identify and extract information\n",
        "        # Based on inspecting github.com/topics:\n",
        "        # Titles are in: <p class=\"f3 lh-condensed mb-0 mt-1 Link--primary\">\n",
        "        # Descriptions are in: <p class=\"f5 color-fg-muted mb-0 mt-1\">\n",
        "\n",
        "        title_tags = soup.find_all('p', class_='f3 lh-condensed mb-0 mt-1 Link--primary')\n",
        "        desc_tags = soup.find_all('p', class_='f5 color-fg-muted mb-0 mt-1')\n",
        "\n",
        "        topic_titles = [tag.get_text().strip() for tag in title_tags]\n",
        "        topic_descriptions = [tag.get_text().strip() for tag in desc_tags]\n",
        "\n",
        "        # 2.3. Print length and content of extracted lists\n",
        "        print(\"\\n--- Extracted Titles ---\")\n",
        "        print(f\"Found {len(topic_titles)} titles.\")\n",
        "        for i, title in enumerate(topic_titles[:5]): # Print first 5 for brevity\n",
        "            print(f\"  {i+1}. {title}\")\n",
        "\n",
        "        print(\"\\n--- Extracted Descriptions ---\")\n",
        "        print(f\"Found {len(topic_descriptions)} descriptions.\")\n",
        "        for i, desc in enumerate(topic_descriptions[:5]): # Print first 5 for brevity\n",
        "            print(f\"  {i+1}. {desc}\")\n",
        "\n",
        "        return topic_titles, topic_descriptions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing HTML file: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_dataframe(titles, descriptions):\n",
        "    \"\"\"Creates a pandas DataFrame from the extracted data.\"\"\"\n",
        "    print(\"\\n--- 3. Creating pandas DataFrame ---\")\n",
        "\n",
        "    # 3.1. Create dictionary\n",
        "    # Ensure lists are of the same length for the DataFrame\n",
        "    min_length = min(len(titles), len(descriptions))\n",
        "\n",
        "    if len(titles) != len(descriptions):\n",
        "        print(f\"Warning: Mismatch in counts. Titles: {len(titles)}, Descriptions: {len(descriptions)}.\")\n",
        "        print(f\"Truncating to the shorter length: {min_length}\")\n",
        "\n",
        "    data_dict = {\n",
        "        'Title': titles[:min_length],\n",
        "        'Description': descriptions[:min_length]\n",
        "    }\n",
        "\n",
        "    # 3.2. Convert to pandas DataFrame\n",
        "    df = pd.DataFrame(data_dict)\n",
        "\n",
        "    # 3.3. Print the DataFrame\n",
        "    print(\"Successfully created DataFrame:\")\n",
        "    print(df)\n",
        "\n",
        "    print(\"\\nDataFrame Info:\")\n",
        "    df.info()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting GitHub Topics Scraper...\")\n",
        "\n",
        "    # Step 1: Fetch and save HTML\n",
        "    if fetch_and_save_html(URL, HTML_FILE):\n",
        "\n",
        "        # Step 2: Parse the saved HTML\n",
        "        extracted_data = parse_html_file(HTML_FILE)\n",
        "\n",
        "        if extracted_data:\n",
        "            titles, descriptions = extracted_data\n",
        "\n",
        "            # Step 3: Create and display the DataFrame\n",
        "            create_dataframe(titles, descriptions)\n",
        "\n",
        "    print(\"\\nScraping process complete.\")"
      ]
    }
  ]
}